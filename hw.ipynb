{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import pandas as pd\n",
    "import json,os,sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ajId`:ID of the case,   \n",
    "`ajName` is the case name,   \n",
    "`ajjbqk` 基本事实,  \n",
    "`pjjg` 判决,    \n",
    "`qw`  全部内容,   \n",
    "`writId` 文档ID,   \n",
    "`writName` 文档名称   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "charges = json.load(open(\"data/documents/common_charge.json\",encoding=\"utf-8\"))\n",
    "query_related = {v for key in charges for v in charges[key][:100] if v.endswith(\".json\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8824"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_related)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建倒排索引\n",
    "1. 分词和去除停用词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import norm\n",
    "from typing import List,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"index.db\")\n",
    "cur = conn.cursor()\n",
    "matrix = sparse.load_npz(\"./document_matrix.npz\")\n",
    "matrix = matrix.T\n",
    "cur.execute(\"\"\"select * from words\"\"\")\n",
    "words = [word[1] for word in cur.fetchall()]\n",
    "cur.execute(\"\"\"select * from dc\"\"\")\n",
    "word_doc_count = [dc[1] for dc in cur.fetchall()]\n",
    "words_idx = {word:idx for idx,word in enumerate(words)}\n",
    "documents_length = cur.execute(\"\"\"select count(*) from cases\"\"\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_matrix(keys:list):\n",
    "\tl = []\n",
    "\tfor key in keys:\n",
    "\t\tif key in words:\n",
    "\t\t\tl.append(words_idx[key])\n",
    "\tsql = \"\"\"select document from inverse_index where word_index in ({})\"\"\".format(\",\".join([\"?\"]*len(l)) )\n",
    "\tcur.execute(sql,l)\n",
    "\treturn np.array(list({i[0] for i in cur.fetchall()}))\n",
    "\t\n",
    "def query(key:str,n:int=20):\n",
    "\tkey_with_count = pd.Series(list(jieba.cut(key))).value_counts().to_dict()\n",
    "\t_x = sparse.lil_matrix((len(words),1))\n",
    "\t# 生成查询向量的tf-idf\n",
    "\tfor word in key_with_count:\n",
    "\t\tif word in words:\n",
    "\t\t\tpos = words_idx[word]\n",
    "\t\t\t_x[pos,0] = key_with_count[word] * np.log( (documents_length+1) / (1 + word_doc_count[pos]))\n",
    "\tx = _x.tocsr()\n",
    "\t# 分词，利用倒排索引\n",
    "\tdocuement_to_query = get_documents_matrix(key_with_count.keys())\n",
    "\tm = matrix[docuement_to_query]\n",
    "\t# 计算查询向量与文档向量的余弦相似度\n",
    "\tcosine_distance = (m@x).toarray().flatten()/(norm(m,axis=1) * norm(x))\n",
    "\tind = np.argpartition(cosine_distance, -n)[-n:]\n",
    "\treturn docuement_to_query[ind[np.argsort(cosine_distance[ind])]]\n",
    "\n",
    "def get_documents(document_ids:int):\n",
    "\tcommand = f\"\"\"select qw from cases where id in ({\",\".join([\"?\"]*len(document_ids))})\"\"\"\n",
    "\tcur.execute(command,document_ids)\n",
    "\treturn [i[0] for i in cur.fetchall()]\n",
    "\n",
    "class DocumentKeywordPos:\n",
    "\tdef __init__(self,start:int,end:int):\n",
    "\t\tself.start = start\n",
    "\t\tself.end = end\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"{self.start}:{self.end}\"\n",
    "def get_document_with_hightlight(document_ids,keys:List[str])->Tuple[str,List[DocumentKeywordPos]]:\n",
    "\tre_expr = \"|\".join([\"({})\".format(re.escape(key)) for key in keys])\n",
    "\tret = []\n",
    "\tfor document in get_documents(document_ids):\n",
    "\t\tret.append((document,[DocumentKeywordPos(match.start(),match.end()) for match in re.finditer(re_expr, document)]))\n",
    "\treturn ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_with_count = pd.Series(list(jieba.cut(\"计算机\"))).value_counts().to_dict()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d44dfae6451dbe052b0ffacaf8bd4295658ed94f6e76d6cefe365a8f1afb967"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cs224n')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

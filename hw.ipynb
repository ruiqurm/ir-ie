{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ajId`:ID of the case,   \n",
    "`ajName` is the case name,   \n",
    "`ajjbqk` 基本事实,  \n",
    "`pjjg` 判决,    \n",
    "`qw`  全部内容,   \n",
    "`writId` 文档ID,   \n",
    "`writName` 文档名称   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据\n",
    "data = []\n",
    "for directory, dirnames, filenames in os.walk(r'LeCaRD/data/candidates'):\n",
    "\tfor name in filenames:\n",
    "\t\tjson_file = os.path.join(directory, name)\n",
    "\t\tdata.append(json.load(open(json_file,encoding=\"utf-8\")))\n",
    "data = pd.DataFrame(data)\n",
    "data = data[[\"qw\"]].to_dict()[\"qw\"]\n",
    "data = [data[i] for i in range(len(data.keys()))]\n",
    "documents = data # 换个名字\n",
    "\n",
    "# 读取停用词\n",
    "with open(\"LeCaRD/data/others/stopword.txt\",encoding=\"utf-8\") as f:\n",
    "\tstopwords = set(f.read().split(\"\\n\"))\n",
    "stopwords.add(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建倒排索引\n",
    "1. 分词和去除停用词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import norm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"index.db\")\n",
    "cur = conn.cursor()\n",
    "matrix = sparse.load_npz(\"./document_matrix.npz\")\n",
    "matrix = matrix.T\n",
    "cur.execute(\"\"\"select * from words\"\"\")\n",
    "words = [word[1] for word in cur.fetchall()]\n",
    "cur.execute(\"\"\"select * from dc\"\"\")\n",
    "word_doc_count = [dc[1] for dc in cur.fetchall()]\n",
    "words_idx = {word:idx for idx,word in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_matrix(keys:list):\n",
    "\tl = []\n",
    "\tfor key in keys:\n",
    "\t\tif key in words:\n",
    "\t\t\tl.append(words_idx[key])\n",
    "\tsql = \"\"\"select document from inverse_index where word_index in ({})\"\"\".format(\",\".join([\"?\"]*len(l)))\n",
    "\tcur.execute(sql,l)\n",
    "\treturn np.array(list({i[0] for i in cur.fetchall()}))\n",
    "\t\n",
    "\t\n",
    "# def get_document(word):\n",
    "# \tif word in words:\n",
    "# \t\tword_index = words.index(word)\n",
    "# \telse:\n",
    "# \t\treturn []\n",
    "# \tcur.execute(\"\"\"select document from inverse_index where word_index=?\"\"\",(word_index,))\n",
    "# \treturn [i[0] for i in cur.fetchall()]\n",
    "\n",
    "\n",
    "def query(key:str,n:int=20):\n",
    "\tkey_with_count = pd.Series(list(jieba.cut(key))).value_counts().to_dict()\n",
    "\t_x = sparse.lil_matrix((len(words),1))\n",
    "\t# 生成查询向量的tf-idf\n",
    "\tfor word in key_with_count:\n",
    "\t\tif word in words:\n",
    "\t\t\tpos = words_idx[word]\n",
    "\t\t\t_x[pos,0] = key_with_count[word] * np.log( (len(documents)+1) / (1 + word_doc_count[pos]))\n",
    "\tx = _x.tocsr()\n",
    "\t# 分词，利用倒排索引\n",
    "\tdocuement_to_query = get_documents(key_with_count.keys())\n",
    "\tm = matrix[docuement_to_query]\n",
    "\t# 计算查询向量与文档向量的余弦相似度\n",
    "\tcosine_distance = (m@x).toarray().flatten()/(norm(m,axis=1) * norm(x))\n",
    "\tind = np.argpartition(cosine_distance, -n)[-n:]\n",
    "\treturn docuement_to_query[ind[np.argsort(cosine_distance[ind])]]\n",
    "\n",
    "def get_document(document_id:int):\n",
    "\treturn documents[document_id]\n",
    "\n",
    "class DocumentKeywordPos:\n",
    "\tdef __init__(self,start:int,end:int):\n",
    "\t\tself.start = start\n",
    "\t\tself.end = end\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"{self.start}:{self.end}\"\n",
    "def get_document_with_hightlight(document_id,keys:List[str])->Tuple[str,List[DocumentKeywordPos]]:\n",
    "\tre_expr = \"|\".join([\"({})\".format(re.escape(key)) for key in keys])\n",
    "\tdocument = get_document(document_id)\n",
    "\thightlight = [DocumentKeywordPos(match.start(),match.end()) for match in re.finditer(re_expr, document)]\n",
    "\treturn document,hightlight"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d44dfae6451dbe052b0ffacaf8bd4295658ed94f6e76d6cefe365a8f1afb967"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cs224n')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
